# ğŸš€ CHUNKED TRAINING ON GOOGLE COLAB

## HÆ°á»›ng dáº«n cháº¡y Chunked Training trÃªn Google Colab

### ğŸ“‹ Chuáº©n bá»‹:

1. **CÃ¡c file cáº§n upload lÃªn Colab**:
   ```
   â”œâ”€â”€ src/train_slm_qwen_lora_v4_chunked.py
   â”œâ”€â”€ data/slm_train_style_adapted.jsonl  (154,477 samples)
   â””â”€â”€ data/slm_val.jsonl
   ```

2. **Hoáº·c clone tá»« GitHub** (khuyáº¿n nghá»‹):
   - Push code lÃªn GitHub
   - Clone trong Colab

---

## ğŸ¯ Option 1: Upload files trá»±c tiáº¿p

### Step 1: Táº¡o Colab Notebook má»›i
Go to: https://colab.research.google.com/

### Step 2: Chá»n GPU Runtime
- Runtime â†’ Change runtime type â†’ GPU (T4)
- Náº¿u cÃ³ Pro: chá»n A100 hoáº·c V100 (nhanh hÆ¡n)

### Step 3: CÃ i Ä‘áº·t dependencies

```python
# Cell 1: Install dependencies
!pip install -q transformers peft datasets accelerate bitsandbytes
```

### Step 4: Upload files

```python
# Cell 2: Upload training script vÃ  data
from google.colab import files
import os

# Táº¡o thÆ° má»¥c
!mkdir -p /content/src
!mkdir -p /content/data
!mkdir -p /content/models

print("ğŸ“ Upload train_slm_qwen_lora_v4_chunked.py vÃ o /content/src/")
uploaded = files.upload()

print("ğŸ“ Upload slm_train_style_adapted.jsonl vÃ o /content/data/")
uploaded = files.upload()

print("âœ“ Upload completed!")
```

### Step 5: Cháº¡y training

```python
# Cell 3: Run chunked training
!cd /content && python src/train_slm_qwen_lora_v4_chunked.py
```

---

## ğŸ¯ Option 2: Clone tá»« GitHub (Khuyáº¿n nghá»‹) â­

### Step 1: Push code lÃªn GitHub

```bash
# TrÃªn local machine
cd /home/phuc/workspace/school/medical-data-mining-project

# Add files
git add src/train_slm_qwen_lora_v4_chunked.py
git add CHUNKED_TRAINING_LOG.md
git add FULL_DATASET_TRAINING.md

# Commit
git commit -m "Add chunked training for full dataset"

# Push
git push origin main
```

### Step 2: Clone trong Colab

```python
# Cell 1: Clone repository
!git clone https://github.com/phucfix/medical-data-mining.git
%cd medical-data-mining
```

### Step 3: CÃ i dependencies

```python
# Cell 2: Install dependencies
!pip install -q transformers peft datasets accelerate bitsandbytes torch
```

### Step 4: Check data

```python
# Cell 3: Verify data files
import os
print("Checking data files...")
print(f"Train file exists: {os.path.exists('data/slm_train_style_adapted.jsonl')}")
print(f"Val file exists: {os.path.exists('data/slm_val.jsonl')}")

# Check file size
if os.path.exists('data/slm_train_style_adapted.jsonl'):
    size_mb = os.path.getsize('data/slm_train_style_adapted.jsonl') / (1024*1024)
    print(f"Train file size: {size_mb:.2f} MB")
```

### Step 5: Run training

```python
# Cell 4: Run chunked training
!python src/train_slm_qwen_lora_v4_chunked.py
```

---

## ğŸ“Š Monitoring trong Colab

Báº¡n sáº½ tháº¥y output nhÆ° nÃ y:

```
================================================================================
CHUNKED TRAINING - TRAIN FULL 154K SAMPLES WITHOUT OOM
================================================================================

âœ“ GPU: Tesla T4
  Memory: 14.74 GB

Loading full training data...
âœ“ Total samples: 154477

âœ“ Split into 6 chunks:
  Chunk 1: 30000 samples
  Chunk 2: 30000 samples
  Chunk 3: 30000 samples
  Chunk 4: 30000 samples
  Chunk 5: 30000 samples
  Chunk 6: 24477 samples

================================================================================
TRAINING CHUNK 1/6
Samples in this chunk: 30000
================================================================================
Loading fresh base model: Qwen/Qwen2.5-0.5B-Instruct
...
â–¶ Training chunk 1...
{'loss': 0.5234, 'learning_rate': 2e-05, 'epoch': 0.5}
...
âœ“ Chunk 1/6 completed
  Progress: 16.7%

================================================================================
TRAINING CHUNK 2/6
Samples in this chunk: 30000
================================================================================
Loading model from previous chunk...
âœ“ Loaded LoRA weights from previous chunk
...
```

---

## â±ï¸ Thá»i gian dá»± kiáº¿n:

### TrÃªn T4 (Free Colab):
- **Má»—i chunk**: ~25-30 phÃºt
- **Total**: 2.5-3 giá»
- **LÆ°u Ã½**: Free Colab cÃ³ giá»›i háº¡n ~12 giá» session

### TrÃªn A100 (Colab Pro):
- **Má»—i chunk**: ~10-15 phÃºt
- **Total**: 1-1.5 giá»
- **Khuyáº¿n nghá»‹**: Náº¿u cÃ³ Pro, dÃ¹ng A100!

---

## ğŸ’¾ Download model sau khi train xong

```python
# Cell 5: Zip vÃ  download model
!zip -r qwen_v4_chunked.zip models/qwen2.5-0.5b-med-slm-lora-v4-chunked/

from google.colab import files
files.download('qwen_v4_chunked.zip')
```

---

## ğŸ”§ Troubleshooting

### 1. Session timeout (Free Colab)
**Giáº£i phÃ¡p**: 
```python
# ThÃªm vÃ o cell Ä‘áº§u Ä‘á»ƒ keep session alive
import IPython
display(IPython.display.Javascript('''
 function KeepClicking(){
   console.log("Clicking");
   document.querySelector("colab-connect-button").click()
 }
 setInterval(KeepClicking,60000)
'''))
```

### 2. Data file quÃ¡ lá»›n (>100MB)
**Giáº£i phÃ¡p**:
- Upload lÃªn Google Drive
- Mount Drive trong Colab:

```python
from google.colab import drive
drive.mount('/content/drive')

# Copy data tá»« Drive
!cp /content/drive/MyDrive/medical-data/*.jsonl /content/data/
```

### 3. OOM trÃªn T4
**Giáº£i phÃ¡p**: Script Ä‘Ã£ optimize, nhÆ°ng náº¿u váº«n OOM:
- Giáº£m `CHUNK_SIZE` tá»« 30000 â†’ 20000
- Hoáº·c upgrade lÃªn Colab Pro (A100)

---

## ğŸ“ Complete Colab Notebook Template

TÃ´i Ä‘Ã£ táº¡o file notebook hoÃ n chá»‰nh: `COLAB_CHUNKED_TRAINING.ipynb`

Chá»‰ cáº§n:
1. Upload notebook lÃªn Colab
2. Click Runtime â†’ Run all
3. Chá» 2.5-3 giá»
4. Download model

---

## âœ… Sau khi training xong

1. **Download model** tá»« Colab vá» local
2. **Extract zip file**
3. **Test trÃªn Test_sample.v1.0.csv**:
   ```bash
   python src/test_qwen_on_sample_v3.py
   ```
4. **Ká»³ vá»ng**: 85-90% accuracy! ğŸ‰

---

## ğŸ¯ TÃ³m táº¯t workflow:

```
1. Push code lÃªn GitHub
   â†“
2. Má»Ÿ Google Colab, chá»n GPU
   â†“
3. Clone repo tá»« GitHub
   â†“
4. Install dependencies
   â†“
5. Run chunked training (2.5-3h)
   â†“
6. Download model
   â†“
7. Test vÃ  Ä‘áº¡t 85-90% accuracy! ğŸš€
```

Báº¡n muá»‘n tÃ´i táº¡o file notebook Colab ready-to-use khÃ´ng? Hoáº·c báº¡n sáº½ lÃ m manual theo hÆ°á»›ng dáº«n trÃªn?
