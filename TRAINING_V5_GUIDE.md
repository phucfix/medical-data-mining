# V5 Optimized Training - Giá»›i háº¡n â‰¤1B Parameters

## ğŸ¯ Má»¥c tiÃªu
- Cáº£i thiá»‡n accuracy tá»« **51%** (v4-chunked) lÃªn **60-65%** 
- Giá»¯ model size â‰¤ 1B parameters (Qwen2.5-0.5B = 494M)
- KhÃ´ng cáº§n upgrade model size, chá»‰ optimize training

## âš™ï¸ Cáº£i tiáº¿n chÃ­nh

### 1. **TÄƒng sá»‘ epochs** â±ï¸
- **v4**: 1 epoch per chunk (6 chunks total)
- **v5**: 3 epochs full dataset
- **LÃ½ do**: Model chÆ°a há»c Ä‘á»§, cáº§n nhiá»u epochs hÆ¡n

### 2. **Giáº£m Learning Rate** ğŸ“‰
- **v4**: 2e-5 (fast but unstable)
- **v5**: 5e-6 (slow but stable)
- **LÃ½ do**: LR tháº¥p hÆ¡n â†’ há»c cháº­m hÆ¡n nhÆ°ng chÃ­nh xÃ¡c hÆ¡n, trÃ¡nh overshoot

### 3. **Cosine Scheduler + Longer Warmup** ğŸ“Š
- **v4**: Warmup ratio = 0.05
- **v5**: Warmup ratio = 0.1 + cosine decay
- **LÃ½ do**: Warmup dÃ i hÆ¡n â†’ stable start, cosine decay â†’ smooth convergence

### 4. **TÄƒng LoRA Rank** ğŸš€
- **v4**: rank=32, alpha=64
- **v5**: rank=64, alpha=128
- **LÃ½ do**: Model capacity cao hÆ¡n â†’ há»c Ä‘Æ°á»£c patterns phá»©c táº¡p hÆ¡n

### 5. **Gradient Clipping** ğŸ›¡ï¸
- **v5**: max_grad_norm=1.0
- **LÃ½ do**: Prevent gradient explosion, training stability

### 6. **Smart Evaluation** ğŸ“ˆ
- **v4**: No evaluation during training
- **v5**: Eval every 2000 steps, save best checkpoint
- **LÃ½ do**: Monitor training progress, prevent overfitting

## ğŸ“Š So sÃ¡nh Config

| Parameter | v4-chunked | v5-optimized | Change |
|-----------|------------|--------------|--------|
| Epochs | 1 per chunk | 3 full | +200% |
| Learning Rate | 2e-5 | 5e-6 | -75% |
| LR Scheduler | Linear | Cosine | Better |
| Warmup Ratio | 0.05 | 0.1 | +100% |
| LoRA Rank | 32 | 64 | +100% |
| LoRA Alpha | 64 | 128 | +100% |
| Gradient Clip | None | 1.0 | Added |
| Eval Strategy | No | Every 2000 steps | Added |

## ğŸ¯ Expected Results

| Version | Strategy | Accuracy | Improvement |
|---------|----------|----------|-------------|
| v2-merged | 50k samples, data leakage | 69% | âš ï¸ Inflated |
| v4-chunked | 154k, 1 epoch chunks | 51% | Baseline |
| **v5-optimized** | **154k, 3 epochs, optimized** | **60-65%** | **+9-14%** |

## â±ï¸ Training Time
- **v4-chunked**: ~2.5-3 hours (6 chunks Ã— 25-35 min)
- **v5-optimized**: ~4-5 hours (3 epochs full dataset)
- **Hardware**: Google Colab T4 GPU (15GB VRAM)

## ğŸ“ CÃ¡ch sá»­ dá»¥ng

### Option 1: Local Training (náº¿u cÃ³ GPU)
```bash
python src/train_slm_qwen_lora_v5_optimized.py
```

### Option 2: Google Colab (Recommended)
1. Upload `V5_Optimized_Training_Colab.ipynb` lÃªn Colab
2. Chá»n Runtime > Change runtime type > T4 GPU
3. Run all cells
4. Äá»£i ~4-5 hours
5. Download model khi xong

## ğŸ”¬ Táº¡i sao khÃ´ng upgrade lÃªn 1.5B?

**User constraint**: Chá»‰ Ä‘Æ°á»£c dÃ¹ng model â‰¤ 1B parameters

**Available options**:
- âœ… Qwen2.5-0.5B (494M) - Ä‘ang dÃ¹ng
- âŒ Qwen2.5-1.5B (1.54B) - vÆ°á»£t giá»›i háº¡n
- âš ï¸ TinyLlama-1.1B (1.1B) - cÃ³ thá»ƒ thá»­ nhÆ°ng kÃ©m hÆ¡n Qwen

**Solution**: Optimize training thay vÃ¬ upgrade model

## ğŸ“ˆ Analysis: Táº¡i sao v4 chá»‰ 51%?

### 1. **Underfitting** (chÆ°a há»c Ä‘á»§)
- v4 train 1 epoch per chunk = effectively 1 epoch total
- Medical domain phá»©c táº¡p â†’ cáº§n nhiá»u epochs hÆ¡n
- **Fix v5**: 3 epochs

### 2. **Learning Rate quÃ¡ cao**
- LR=2e-5 cÃ³ thá»ƒ skip qua optimal points
- **Fix v5**: LR=5e-6 (slow but precise)

### 3. **LoRA capacity tháº¥p**
- Rank=32 cÃ³ thá»ƒ khÃ´ng Ä‘á»§ cho medical domain
- **Fix v5**: Rank=64 (2x capacity)

### 4. **Chunked training issues**
- Má»—i chunk train riÃªng biá»‡t â†’ khÃ´ng ideal
- **Fix v5**: Train full dataset liÃªn tá»¥c

### 5. **No monitoring**
- v4 khÃ´ng cÃ³ eval â†’ khÃ´ng biáº¿t training progress
- **Fix v5**: Eval every 2000 steps

## ğŸ“ Key Learnings

1. **Model size khÃ´ng pháº£i everything**: v4 cÃ³ 3x data (154k vs 50k) nhÆ°ng chá»‰ tÄƒng 2% (49%â†’51%)
   - â†’ Bottleneck khÃ´ng pháº£i data, mÃ  lÃ  **training strategy**

2. **Chunked training khÃ´ng tá»‘i Æ°u**: Chia nhá» chunks â†’ máº¥t continuity
   - â†’ Train full dataset liÃªn tá»¥c tá»‘t hÆ¡n

3. **1 epoch khÃ´ng Ä‘á»§ cho medical domain**: Cáº§n Ã­t nháº¥t 3 epochs
   - â†’ Medical QA phá»©c táº¡p hÆ¡n general text

4. **LoRA rank matters**: Rank cÃ ng cao â†’ capacity cÃ ng lá»›n
   - â†’ NhÆ°ng khÃ´ng nÃªn quÃ¡ cao (risk overfitting)

## ğŸš€ Next Steps

### Sau khi train v5:
1. **Test model**: `python src/test_qwen_on_sample_v4.py --version v5-optimized`
2. **Compare results**: v4 (51%) vs v5 (expected 60-65%)

### Náº¿u v5 váº«n chÆ°a Ä‘á»§ (< 60%):
- **Option A**: Thá»­ TinyLlama-1.1B (gáº§n 1B limit)
- **Option B**: Cáº£i thiá»‡n data quality:
  - Augment thÃªm medical examples
  - Filter low-quality samples
  - Balance TRUE/FALSE distribution
- **Option C**: Ensemble methods:
  - Train multiple v5 models vá»›i different seeds
  - Voting mechanism

### Náº¿u v5 Ä‘áº¡t 60-65%:
- âœ… Success! ÄÃ£ cáº£i thiá»‡n +9-14% trong constraints
- ğŸ“Š Analyze error cases
- ğŸ¯ Fine-tune trÃªn specific medical subdomains

## ğŸ“š Files Created

- `src/train_slm_qwen_lora_v5_optimized.py` - Training script
- `V5_Optimized_Training_Colab.ipynb` - Colab notebook
- `TRAINING_V5_GUIDE.md` - This guide

## ğŸ¤ Comparison vá»›i alternatives

| Approach | Pros | Cons | Expected Gain |
|----------|------|------|---------------|
| **v5 Optimized** | âœ… Simple, stays within 1B limit | âš ï¸ Still 0.5B capacity | **+9-14%** |
| Upgrade to 1.5B | Higher capacity | âŒ Violates â‰¤1B constraint | N/A |
| TinyLlama 1.1B | Almost at limit | âš ï¸ Lower quality than Qwen | +5-10%? |
| Data augmentation | More training data | Diminishing returns (154k already large) | +2-5% |
| Ensemble | Best accuracy | Complex deployment | +3-7% |

**Recommendation**: Try v5 first! Náº¿u khÃ´ng Ä‘á»§ thÃ¬ consider TinyLlama-1.1B.

---

**Created**: 2025-12-09  
**Author**: GitHub Copilot  
**Status**: Ready for training ğŸš€
