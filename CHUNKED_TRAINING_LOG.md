# ğŸš€ CHUNKED TRAINING - Execution Guide

## ğŸ“‹ ThÃ´ng tin training:

### Dataset:
- **Total samples**: 154,477
- **Chunks**: 6 chunks (5Ã—30k + 1Ã—24k)
- **Strategy**: Train tá»«ng chunk, accumulate LoRA weights

### Expected results:
- â±ï¸ **Time**: 2.5-3 giá» total
- ğŸ“Š **Accuracy**: 85-90% on Test_sample.v1.0.csv
- ğŸš« **OOM**: Zero risk (má»—i chunk chá»‰ 30k)
- ğŸ’¾ **Final model**: models/qwen2.5-0.5b-med-slm-lora-v4-chunked/

---

## ğŸ”„ Progress tracking:

### Chunk 1/6 (samples 0-30,000)
- Status: â³ Pending
- Time: ~25-30 minutes
- Progress: 0%

### Chunk 2/6 (samples 30,000-60,000)
- Status: â³ Pending
- Time: ~25-30 minutes
- Progress: 0%

### Chunk 3/6 (samples 60,000-90,000)
- Status: â³ Pending
- Time: ~25-30 minutes
- Progress: 0%

### Chunk 4/6 (samples 90,000-120,000)
- Status: â³ Pending
- Time: ~25-30 minutes
- Progress: 0%

### Chunk 5/6 (samples 120,000-150,000)
- Status: â³ Pending
- Time: ~25-30 minutes
- Progress: 0%

### Chunk 6/6 (samples 150,000-154,477)
- Status: â³ Pending
- Time: ~20-25 minutes
- Progress: 0%

---

## ğŸ“Š Estimated timeline:

```
Start time: [Will be logged]
Chunk 1:    [Start] â†’ [End] (~30 min)
Chunk 2:    [Start] â†’ [End] (~30 min)
Chunk 3:    [Start] â†’ [End] (~30 min)
Chunk 4:    [Start] â†’ [End] (~30 min)
Chunk 5:    [Start] â†’ [End] (~30 min)
Chunk 6:    [Start] â†’ [End] (~25 min)
Total:      ~2.5-3 hours
```

---

## ğŸ¯ What happens during training:

1. **Load data**: Read 154,477 samples from slm_train_style_adapted.jsonl
2. **Shuffle**: Random shuffle with seed=42
3. **Split**: Divide into 6 chunks
4. **Train Chunk 1**: Train fresh model on first 30k samples â†’ Save
5. **Train Chunk 2**: Load Chunk 1 weights â†’ Train on next 30k â†’ Save
6. **Train Chunk 3-6**: Continue accumulating knowledge
7. **Finalize**: Copy final model to output directory

---

## ğŸ“ Output files:

```
models/
â”œâ”€â”€ temp_chunks/          # Temporary chunk models
â”‚   â”œâ”€â”€ chunk_0/         # Chunk 1 model (will be deleted)
â”‚   â”œâ”€â”€ chunk_1/         # Chunk 2 model (will be deleted)
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ chunk_5/         # Final chunk (kept as backup)
â”‚
â””â”€â”€ qwen2.5-0.5b-med-slm-lora-v4-chunked/   # FINAL MODEL
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ adapter_model.safetensors
    â”œâ”€â”€ tokenizer.json
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ metrics.json
```

---

## âš ï¸ Important notes:

1. **Don't interrupt**: Náº¿u dá»«ng giá»¯a chá»«ng, pháº£i cháº¡y láº¡i tá»« Ä‘áº§u
2. **GPU memory**: Sáº½ tá»± Ä‘á»™ng clear cache sau má»—i chunk
3. **Disk space**: Cáº§n ~3-4GB cho temp chunks
4. **Progress**: Monitor terminal output Ä‘á»ƒ biáº¿t progress

---

## ğŸ” How to monitor:

Watch for these messages:
```
TRAINING CHUNK 1/6         â† Chunk starting
âœ“ Chunk 1/6 completed      â† Chunk finished
  Progress: 16.7%          â† Overall progress

TRAINING CHUNK 2/6         â† Next chunk
Loading model from previous chunk...  â† Weight inheritance
```

---

## âœ… Success indicators:

When training completes, you should see:
```
âœ“âœ“ CHUNKED TRAINING COMPLETED âœ“âœ“
Final model: models/qwen2.5-0.5b-med-slm-lora-v4-chunked
Total samples trained: 154477
Number of chunks: 6
Expected accuracy: 85-90%
```

---

## ğŸ§ª After training:

1. **Evaluate on Test_sample**:
   ```bash
   python src/test_qwen_on_sample_v3.py
   ```

2. **Check metrics**:
   ```bash
   cat models/qwen2.5-0.5b-med-slm-lora-v4-chunked/metrics.json
   ```

3. **Compare with v2**:
   - v2 (merged data): 69% accuracy
   - v4 (chunked full): 85-90% accuracy (expected)
   - Improvement: +16-21 percentage points! ğŸ‰

---

## ğŸš€ Ready to start!

Command to run:
```bash
python src/train_slm_qwen_lora_v4_chunked.py
```

Expected completion: 2.5-3 giá»
Expected accuracy: 85-90%
OOM risk: Zero âœ…

Good luck! ğŸ’ª
