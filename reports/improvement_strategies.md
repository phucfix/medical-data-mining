# CHIáº¾N LÆ¯á»¢C TÄ‚NG Äá»˜ CHÃNH XÃC CHO MODEL Y Táº¾

## ğŸ“Š Hiá»‡n tráº¡ng: 69% accuracy â†’ Má»¥c tiÃªu: 80-85%+

---

## 1ï¸âƒ£ Cáº¢I THIá»†N Dá»® LIá»†U (Data-Centric Approach)

### ğŸ¯ A. TÄƒng sá»‘ lÆ°á»£ng dá»¯ liá»‡u training (Impact: â­â­â­â­â­)

#### PhÆ°Æ¡ng Ã¡n 1: Merge toÃ n bá»™ Test_sample vÃ o training
```python
# Thay vÃ¬ 50/50, dÃ¹ng 80/20 hoáº·c 90/10
TEST_SAMPLE_TRAIN_RATIO = 0.8  # 80% Ä‘á»ƒ train, 20% Ä‘á»ƒ test
```
**Expected improvement**: +5-10%

#### PhÆ°Æ¡ng Ã¡n 2: Data augmentation thÃ´ng minh
```python
# Paraphrase cÃ¢u há»i vá»›i nhiá»u cÃ¡ch diá»…n Ä‘áº¡t khÃ¡c nhau
Original: "Insulin Ä‘Æ°á»£c sáº£n xuáº¥t bá»Ÿi tuyáº¿n tá»¥y."
Aug 1:    "Tuyáº¿n tá»¥y lÃ  cÆ¡ quan sáº£n xuáº¥t insulin."
Aug 2:    "Insulin cÃ³ nguá»“n gá»‘c tá»« tuyáº¿n tá»¥y."
Aug 3:    "Tuyáº¿n tá»¥y chá»‹u trÃ¡ch nhiá»‡m sáº£n xuáº¥t hormone insulin."
```
**Tools**: 
- Back-translation (VN â†’ EN â†’ VN)
- Paraphrase vá»›i GPT-4/Gemini
- Vietnamese NLP tools (VnCoreNLP)

**Expected improvement**: +3-5%

#### PhÆ°Æ¡ng Ã¡n 3: Active learning
```python
# 1. Test model trÃªn unlabeled data
# 2. Chá»n nhá»¯ng cÃ¢u model "khÃ´ng cháº¯c cháº¯n" (confidence < 0.7)
# 3. Human labeling cho nhá»¯ng cÃ¢u nÃ y
# 4. ThÃªm vÃ o training set
```
**Expected improvement**: +5-8%

---

### ğŸ§¹ B. Cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»¯ liá»‡u (Impact: â­â­â­â­)

#### Chiáº¿n lÆ°á»£c 1: Data cleaning aggressive
```python
# Loáº¡i bá» dá»¯ liá»‡u nhiá»…u
def filter_low_quality(data):
    filtered = []
    for sample in data:
        # Loáº¡i bá» cÃ¢u quÃ¡ ngáº¯n/dÃ i
        if len(sample['input']) < 30 or len(sample['input']) > 200:
            continue
        
        # Loáº¡i bá» cÃ¢u cÃ³ grammar issues
        if has_grammar_errors(sample['input']):
            continue
        
        # Loáº¡i bá» cÃ¢u cÃ³ factual contradictions
        if check_medical_contradiction(sample['input'], sample['output']):
            continue
            
        filtered.append(sample)
    return filtered
```
**Expected improvement**: +2-4%

#### Chiáº¿n lÆ°á»£c 2: Expert validation
- ThuÃª chuyÃªn gia y táº¿ review 10-20% data
- Focus vÃ o nhá»¯ng cÃ¢u model hay sai
- Sá»­a labels sai vÃ  refine wording
**Expected improvement**: +3-5%

#### Chiáº¿n lÆ°á»£c 3: Hard negative mining
```python
# Táº¡o cÃ¢u FALSE khÃ³ hÆ¡n báº±ng cÃ¡ch:
# 1. Äáº£o ngÆ°á»£c logic trong cÃ¢u TRUE
# 2. Thay tháº¿ 1 chi tiáº¿t quan trá»ng
Original TRUE: "Insulin Ä‘Æ°á»£c sáº£n xuáº¥t bá»Ÿi tuyáº¿n tá»¥y."
Hard FALSE:    "Insulin Ä‘Æ°á»£c sáº£n xuáº¥t bá»Ÿi tuyáº¿n giÃ¡p." (thay tá»¥y â†’ giÃ¡p)
```
**Expected improvement**: +4-7%

---

### ğŸ”„ C. Balance vÃ  diversity (Impact: â­â­â­)

#### CÃ¢n báº±ng Ä‘á»™ khÃ³
```python
# Hiá»‡n táº¡i cÃ³ thá»ƒ cÃ³ bias vá» Ä‘á»™ khÃ³
Easy (50%):   "Tim ngÆ°á»i cÃ³ 4 ngÄƒn."
Medium (30%): "Insulin Ä‘iá»u hÃ²a Ä‘Æ°á»ng huyáº¿t báº±ng cÃ¡ch..."
Hard (20%):   "Trong Ä‘Ã¡i thÃ¡o Ä‘Æ°á»ng type 2, táº¿ bÃ o beta..."
```

#### CÃ¢n báº±ng domain
```python
# Ensure coverage across medical domains
distribution = {
    "Cardiology": 15%,
    "Endocrinology": 15%,
    "Neurology": 15%,
    "Infectious Disease": 15%,
    "Pharmacology": 20%,
    "Anatomy": 10%,
    "Other": 10%
}
```
**Expected improvement**: +3-5%

---

## 2ï¸âƒ£ Cáº¢I THIá»†N MODEL (Model-Centric Approach)

### ğŸ¤– A. Thá»­ model lá»›n hÆ¡n (Impact: â­â­â­â­â­)

#### Option 1: Qwen2.5-1.5B-Instruct
```python
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"  # 1.5B params
# Váº«n < 2B, tÄƒng capacity x3
```
**Expected improvement**: +8-12%
**Trade-off**: Tá»‘n memory/compute hÆ¡n

#### Option 2: Phi-3-mini (3.8B)
```python
MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"  # 3.8B params
# Ráº¥t máº¡nh cho reasoning tasks
```
**Expected improvement**: +10-15%
**Trade-off**: Cáº§n GPU tá»‘t hÆ¡n

#### Option 3: Llama-3.2-1B
```python
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
# Balance tá»‘t giá»¯a size vÃ  performance
```
**Expected improvement**: +5-10%

---

### âš™ï¸ B. Tá»‘i Æ°u hyperparameters (Impact: â­â­â­â­)

#### Grid search cho LoRA
```python
from itertools import product

# CÃ¡c hyperparams cáº§n tune
lora_r_options = [8, 16, 32, 64]
lora_alpha_options = [16, 32, 64]
lora_dropout_options = [0.05, 0.1, 0.15]
learning_rate_options = [1e-5, 2e-5, 5e-5]

# Grid search
best_acc = 0
best_config = None

for r, alpha, dropout, lr in product(
    lora_r_options, lora_alpha_options, 
    lora_dropout_options, learning_rate_options
):
    model = train_with_config(r, alpha, dropout, lr)
    acc = evaluate(model)
    if acc > best_acc:
        best_acc = acc
        best_config = (r, alpha, dropout, lr)
```
**Expected improvement**: +3-6%

#### Thá»­ cÃ¡c learning rate schedules
```python
# Cosine annealing with warm restarts
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=total_steps,
    num_cycles=3  # Multiple restarts
)
```
**Expected improvement**: +2-4%

---

### ğŸ“ C. Advanced training techniques (Impact: â­â­â­â­)

#### 1. Multi-stage training
```python
# Stage 1: Train trÃªn toÃ n bá»™ data vá»›i learning rate cao
train_stage1(lr=5e-5, epochs=2)

# Stage 2: Fine-tune trÃªn hard examples vá»›i lr tháº¥p
hard_examples = get_hard_examples(model, train_data)
train_stage2(hard_examples, lr=1e-5, epochs=2)

# Stage 3: Polish vá»›i data augmentation
augmented_data = augment(hard_examples)
train_stage3(augmented_data, lr=5e-6, epochs=1)
```
**Expected improvement**: +5-8%

#### 2. Curriculum learning
```python
# Train tá»« dá»… Ä‘áº¿n khÃ³
easy_data = filter_by_difficulty(data, level='easy')
medium_data = filter_by_difficulty(data, level='medium')
hard_data = filter_by_difficulty(data, level='hard')

# Epoch 1: Easy only
train(easy_data)
# Epoch 2: Easy + Medium
train(easy_data + medium_data)
# Epoch 3: All data
train(easy_data + medium_data + hard_data)
```
**Expected improvement**: +4-6%

#### 3. Ensemble models
```python
# Train 3-5 models vá»›i different random seeds
models = []
for seed in [42, 123, 456, 789, 2024]:
    model = train_with_seed(seed)
    models.append(model)

# Voting mechanism
def predict_ensemble(text):
    votes = [model.predict(text) for model in models]
    return majority_vote(votes)
```
**Expected improvement**: +5-10%
**Trade-off**: Tá»‘n compute x5

---

## 3ï¸âƒ£ KIáº¾N TRÃšC NÃ‚NG CAO (Advanced Architecture)

### ğŸ§  A. RAG (Retrieval-Augmented Generation) (Impact: â­â­â­â­â­)

```python
from langchain import FAISS, OpenAI
from langchain.chains import RetrievalQA

# 1. Build medical knowledge base
knowledge_base = load_medical_kb()  # 233+ entries
embeddings = create_embeddings(knowledge_base)
vectorstore = FAISS.from_documents(knowledge_base, embeddings)

# 2. RAG pipeline
def predict_with_rag(question):
    # Retrieve relevant context
    relevant_docs = vectorstore.similarity_search(question, k=3)
    
    # Augment prompt with context
    context = "\n".join([doc.page_content for doc in relevant_docs])
    augmented_prompt = f"""
    Context y táº¿ liÃªn quan:
    {context}
    
    CÃ¢u há»i: {question}
    Tráº£ lá»i TRUE/FALSE:
    """
    
    # Generate answer
    return model.generate(augmented_prompt)
```
**Expected improvement**: +10-15%
**Benefit**: Model cÃ³ access Ä‘áº¿n facts chÃ­nh xÃ¡c

---

### ğŸ”— B. Chain-of-Thought prompting (Impact: â­â­â­â­)

```python
# Thay vÃ¬ chá»‰ yÃªu cáº§u TRUE/FALSE, yÃªu cáº§u reasoning
prompt = """
Báº¡n lÃ  bÃ¡c sÄ©. HÃ£y phÃ¢n tÃ­ch cÃ¢u sau:

CÃ¢u há»i: {question}

BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh cÃ¡c khÃ¡i niá»‡m y táº¿ chÃ­nh
BÆ°á»›c 2: PhÃ¢n tÃ­ch tÃ­nh Ä‘Ãºng/sai cá»§a tá»«ng pháº§n
BÆ°á»›c 3: Káº¿t luáº­n TRUE hoáº·c FALSE

Tráº£ lá»i:
"""

# Training data cÅ©ng cáº§n format CoT
{
    "input": "Insulin Ä‘Æ°á»£c sáº£n xuáº¥t bá»Ÿi tuyáº¿n tá»¥y.",
    "output": """
    BÆ°á»›c 1: KhÃ¡i niá»‡m - Insulin (hormone), tuyáº¿n tá»¥y (cÆ¡ quan)
    BÆ°á»›c 2: Insulin Ä‘Æ°á»£c sáº£n xuáº¥t bá»Ÿi táº¿ bÃ o beta trong Ä‘áº£o tá»¥y (islets of Langerhans) á»Ÿ tuyáº¿n tá»¥y
    BÆ°á»›c 3: Káº¿t luáº­n: TRUE
    """
}
```
**Expected improvement**: +8-12%
**Note**: Cáº§n nhiá»u compute hÆ¡n cho generation dÃ i

---

### ğŸ¯ C. Two-stage prediction (Impact: â­â­â­â­)

```python
# Stage 1: Binary classification (TRUE/FALSE)
class BinaryClassifier(nn.Module):
    def __init__(self, base_model):
        self.encoder = base_model
        self.classifier = nn.Linear(hidden_size, 2)  # TRUE/FALSE
    
    def forward(self, input_ids):
        embeddings = self.encoder(input_ids)
        logits = self.classifier(embeddings)
        return logits

# Stage 2: Confidence estimation
class ConfidenceEstimator(nn.Module):
    def estimate_confidence(self, embeddings):
        # Predict how confident the model should be
        return confidence_score

# Final prediction
pred = binary_classifier(text)
conf = confidence_estimator(text)
if conf < 0.6:
    # Use RAG or ensemble for low-confidence predictions
    pred = fallback_prediction(text)
```
**Expected improvement**: +6-10%

---

## 4ï¸âƒ£ Ká»¸ THUáº¬T Háº¬U Xá»¬ LÃ (Post-processing)

### ğŸ” A. Confidence thresholding (Impact: â­â­â­)

```python
def predict_with_confidence(text, threshold=0.7):
    logits = model(text)
    probs = softmax(logits)
    max_prob = max(probs)
    
    if max_prob < threshold:
        # KhÃ´ng cháº¯c cháº¯n â†’ dÃ¹ng fallback
        return ensemble_predict(text)
    else:
        return argmax(probs)
```
**Expected improvement**: +3-5%

---

### ğŸ§ª B. Rule-based post-correction (Impact: â­â­â­)

```python
def post_process_prediction(text, pred):
    # Rule 1: Tá»« khÃ³a "khÃ´ng", "khÃ´ng pháº£i" â†’ likely FALSE
    if "khÃ´ng" in text and pred == "TRUE":
        # Double-check with higher threshold
        if model_confidence(text) < 0.85:
            pred = "FALSE"
    
    # Rule 2: Medical facts tá»« knowledge base
    if check_against_kb(text) != pred:
        # KB says different â†’ trust KB for factual statements
        pred = get_from_kb(text)
    
    # Rule 3: Logic consistency
    if has_contradiction(text):
        pred = "FALSE"
    
    return pred
```
**Expected improvement**: +2-4%

---

## 5ï¸âƒ£ PHÆ¯Æ NG PHÃP Káº¾T Há»¢P (Hybrid Approach)

### ğŸ­ A. Multi-model ensemble (Impact: â­â­â­â­â­)

```python
# Combine different architectures
models = {
    'qwen_small': Qwen2.5-0.5B,
    'qwen_large': Qwen2.5-1.5B,
    'phi3': Phi-3-mini,
    'llama': Llama-3.2-1B
}

def weighted_ensemble(text):
    predictions = {}
    for name, model in models.items():
        pred, conf = model.predict_with_confidence(text)
        predictions[name] = (pred, conf)
    
    # Weighted voting based on confidence
    weighted_vote = sum(conf if pred == 'TRUE' else -conf 
                       for pred, conf in predictions.values())
    
    return 'TRUE' if weighted_vote > 0 else 'FALSE'
```
**Expected improvement**: +10-15%

---

### ğŸ”¬ B. Test-time augmentation (Impact: â­â­â­)

```python
def predict_with_tta(text):
    # Generate variations of input
    variations = [
        text,
        paraphrase(text),
        add_context(text),
        simplify(text)
    ]
    
    # Predict on all variations
    predictions = [model.predict(var) for var in variations]
    
    # Majority vote
    return majority_vote(predictions)
```
**Expected improvement**: +4-6%

---

## ğŸ“Š ROADMAP Æ¯U TIÃŠN

### ğŸš€ Quick Wins (1-3 ngÃ y):
1. **Merge 80% Test_sample vÃ o training** â†’ +5-8%
2. **Tune hyperparameters (LoRA r, lr)** â†’ +3-5%
3. **Rule-based post-processing** â†’ +2-3%
**Tá»•ng**: +10-16% â†’ **79-85% accuracy**

### ğŸ¯ Medium-term (1-2 tuáº§n):
4. **Data augmentation (paraphrase)** â†’ +3-5%
5. **Hard negative mining** â†’ +4-6%
6. **Try larger model (Qwen-1.5B)** â†’ +8-10%
**Tá»•ng**: +15-21% â†’ **84-90% accuracy**

### ğŸ† Advanced (1 thÃ¡ng):
7. **RAG integration** â†’ +10-15%
8. **Ensemble 3-5 models** â†’ +5-8%
9. **Chain-of-Thought training** â†’ +5-8%
**Tá»•ng**: +20-31% â†’ **89-100% accuracy**

---

## ğŸ’¡ KHUYáº¾N NGHá»Š Cá»¤ THá»‚

### Äá»ƒ Ä‘áº¡t 75-80% ngay (trong 3 ngÃ y):
```bash
# 1. Merge more test data
python src/merge_datasets.py --ratio 0.8

# 2. Train vá»›i LoRA tá»‘i Æ°u
python src/train_slm_qwen_lora_v3.py \
    --lora_r 32 \
    --lora_alpha 64 \
    --learning_rate 2e-5 \
    --epochs 5

# 3. Add post-processing rules
python src/test_with_post_processing.py
```

### Äá»ƒ Ä‘áº¡t 85%+ (trong 2 tuáº§n):
- Upgrade to Qwen2.5-1.5B hoáº·c Phi-3
- Implement RAG vá»›i medical KB
- Data augmentation extensive (x5 data)

---

## ğŸ¯ Káº¾T LUáº¬N

**Most effective strategies** (ROI cao nháº¥t):
1. â­â­â­â­â­ TÄƒng model size (Qwen-1.5B/Phi-3): +10-15%
2. â­â­â­â­â­ RAG integration: +10-15%
3. â­â­â­â­â­ Merge more training data: +5-10%
4. â­â­â­â­ Data augmentation: +5-8%
5. â­â­â­â­ Ensemble models: +5-10%

**Realistic target**:
- 1 tuáº§n: **75-80% accuracy**
- 2 tuáº§n: **80-85% accuracy**  
- 1 thÃ¡ng: **85-90% accuracy**

Báº¡n muá»‘n tÃ´i implement cá»¥ thá»ƒ phÆ°Æ¡ng Ã¡n nÃ o khÃ´ng?
