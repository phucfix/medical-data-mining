# ðŸš€ HÆ¯á»šNG DáºªN TRAINING Vá»šI DATA AUGMENTATION

## ðŸ“‹ Tá»”NG QUAN

Pipeline nÃ y sáº½:
1. âœ… Augment dá»¯ liá»‡u training (x2-3 samples)
2. âœ… Train model v3 vá»›i augmented data
3. âœ… Evaluate trÃªn test set gá»‘c (KHÃ”NG bao gá»“m Test_sample.v1.0.csv)
4. âœ… Test cuá»‘i cÃ¹ng trÃªn Test_sample.v1.0.csv

**LÆ°u Ã½ quan trá»ng**: Test_sample.v1.0.csv Ä‘Æ°á»£c giá»¯ hoÃ n toÃ n riÃªng biá»‡t, chá»‰ dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng!

---

## ðŸ“Š STEP 1: DATA AUGMENTATION (Local)

### Cháº¡y augmentation
```bash
# Activate environment
source venv/bin/activate

# Augment training data (52,521 â†’ ~105,000 samples)
python src/augment_data.py

# Káº¿t quáº£: data/slm_train_augmented.jsonl
```

**Thá»i gian**: ~30-60 phÃºt (tÃ¹y thuá»™c back-translation API)

**Output**: 
- `data/slm_train_augmented.jsonl` (~105k samples)

---

## ðŸ“ STEP 2: ORGANIZE AUGMENTED DATA (Local)

### Tá»• chá»©c dataset
```bash
python src/organize_augmented_data.py
```

**Output files**:
- `data/slm_train_augmented.jsonl` - Training set (augmented)
- `data/slm_val_augmented.jsonl` - Validation set (original, NO augmentation)
- `data/slm_test_augmented.jsonl` - Test set (original, NO augmentation)

**Dataset structure**:
```
Training:   ~105,000 samples (augmented)
Validation:    6,565 samples (original)
Test:          6,566 samples (original)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:     ~118,000 samples
```

---

## ðŸ“¦ STEP 3: ZIP & UPLOAD TO COLAB

### Zip files
```bash
# Zip augmented data
zip -r data_augmented.zip data/slm_train_augmented.jsonl \
                          data/slm_val_augmented.jsonl \
                          data/slm_test_augmented.jsonl

# Zip training script
zip -r src_v3.zip src/train_slm_qwen_lora_v3_augmented.py \
                   src/model_qwen.py
```

### Upload to Google Colab
1. Má»Ÿ Google Colab
2. Upload `data_augmented.zip` vÃ  `src_v3.zip`
3. Unzip trong Colab:
```python
!unzip -q data_augmented.zip
!unzip -q src_v3.zip
```

---

## ðŸŽ“ STEP 4: TRAINING ON COLAB

### Setup environment
```python
# Install dependencies
!pip install -q transformers peft datasets accelerate bitsandbytes

# Check GPU
import torch
print(f"GPU available: {torch.cuda.is_available()}")
print(f"GPU name: {torch.cuda.get_device_name(0)}")
```

### Run training
```python
!python src/train_slm_qwen_lora_v3_augmented.py
```

**Training config**:
- Model: Qwen2.5-0.5B-Instruct
- LoRA r: 32, alpha: 64
- Epochs: 5
- Batch size: 8 (effective: 16 with gradient accumulation)
- Learning rate: 2e-5

**Expected time**: 3-5 giá» trÃªn Colab T4 GPU

**Expected results**:
- Internal test accuracy: ~88-92%
- Model size: ~50MB (chá»‰ LoRA weights)

---

## ðŸ’¾ STEP 5: DOWNLOAD MODEL

### Zip trained model
```python
# In Colab
!zip -r qwen_lora_v3_augmented.zip models/qwen2.5-0.5b-med-slm-lora-v3-augmented/

# Download
from google.colab import files
files.download('qwen_lora_v3_augmented.zip')
```

### Extract locally
```bash
# On local machine
unzip qwen_lora_v3_augmented.zip
```

---

## ðŸ§ª STEP 6: EVALUATION (Local)

### Test trÃªn augmented test set (internal)
```bash
python src/evaluate_slm_qwen_v3.py
```

Expected: ~88-92% accuracy

### Test trÃªn Test_sample.v1.0.csv (external - FINAL)
```bash
python src/test_qwen_on_sample_v3.py
```

Expected: ~75-85% accuracy (cáº£i thiá»‡n tá»« 69%)

---

## ðŸ“ˆ Káº¾T QUáº¢ Dá»° KIáº¾N

### Comparison table:

| Version | Training Data | Internal Test | External Test (Test_sample.v1.0) |
|---------|---------------|---------------|-----------------------------------|
| v1      | 52k original  | 85.58%        | 49.76%                           |
| v2      | 53k merged    | -             | 69.0%                            |
| **v3**  | **105k augmented** | **~90%**  | **~75-85%** ðŸŽ¯                   |

### Improvement breakdown:
- Base â†’ v2: +19% (from merging test data)
- v2 â†’ v3: +6-16% (from data augmentation)
- Total improvement: **+25-35%** ðŸš€

---

## ðŸ“ SCRIPTS CREATED

### Data preparation:
1. `src/augment_data.py` - Data augmentation vá»›i 5 techniques
2. `src/organize_augmented_data.py` - Organize augmented datasets

### Training:
3. `src/train_slm_qwen_lora_v3_augmented.py` - Train vá»›i augmented data

### Evaluation:
4. `src/evaluate_slm_qwen_v3.py` - Eval trÃªn internal test
5. `src/test_qwen_on_sample_v3.py` - Eval trÃªn Test_sample.v1.0.csv

---

## ðŸŽ¯ AUGMENTATION TECHNIQUES USED

### 1. Back-translation
```
VN â†’ EN â†’ VN
"Tim cÃ³ 4 ngÄƒn" â†’ "Heart has 4 chambers" â†’ "TrÃ¡i tim cÃ³ 4 buá»“ng"
```

### 2. Paraphrase
```
"A lÃ  B" â†’ "B lÃ  Ä‘áº·c Ä‘iá»ƒm cá»§a A"
```

### 3. Synonym replacement
```
"thuá»‘c" â†’ "dÆ°á»£c pháº©m"
"Ä‘iá»u trá»‹" â†’ "chá»¯a trá»‹"
```

### 4. Add context
```
"Tim cÃ³ 4 ngÄƒn" â†’ "Trong y há»c, tim cÃ³ 4 ngÄƒn"
```

### 5. Simplify
```
"Thuá»‘c khÃ¡ng sinh cÃ³ tÃ¡c dá»¥ng..." â†’ "KhÃ¡ng sinh cÃ³ tÃ¡c dá»¥ng..."
```

---

## âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG

### âœ… DO:
- Augment TRAINING data Ä‘á»ƒ tÄƒng diversity
- Giá»¯ VALIDATION/TEST set nguyÃªn gá»‘c
- Test_sample.v1.0.csv KHÃ”NG Ä‘Æ°á»£c merge vÃ o training
- Chá»‰ dÃ¹ng Test_sample.v1.0.csv Ä‘á»ƒ evaluate cuá»‘i cÃ¹ng

### âŒ DON'T:
- Augment validation/test set (gÃ¢y overfitting)
- Merge Test_sample.v1.0.csv vÃ o train (data leakage)
- Train quÃ¡ nhiá»u epochs trÃªn augmented data

---

## ðŸ› TROUBLESHOOTING

### Issue 1: Out of memory during augmentation
```bash
# Giáº£m batch size trong back-translation
# Hoáº·c chá»‰ dÃ¹ng local augmentation (paraphrase, synonym)
```

### Issue 2: Slow back-translation
```bash
# Skip back-translation, dÃ¹ng cÃ¡c methods khÃ¡c
# Edit src/augment_data.py, comment out back_translate
```

### Issue 3: Colab disconnected
```bash
# Colab cÃ³ thá»ƒ disconnect sau 12h
# Chia training thÃ nh nhiá»u checkpoints nhá»
```

---

## ðŸ“Š MONITORING PROGRESS

### During training (Colab):
```python
# Watch training log
# Accuracy should improve gradually:
# Epoch 1: ~75%
# Epoch 2: ~82%
# Epoch 3: ~87%
# Epoch 4: ~90%
# Epoch 5: ~91%
```

### Check intermediate results:
```python
# Load checkpoint and test
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
    "models/qwen2.5-0.5b-med-slm-lora-v3-augmented/checkpoint-2000"
)
```

---

## ðŸŽ‰ SUCCESS METRICS

### Target achieved if:
- âœ… Internal test accuracy: >85%
- âœ… External test (Test_sample.v1.0.csv): >75%
- âœ… Improvement over v2: +6% minimum
- âœ… Model size: <100MB
- âœ… Training time: <6 hours

---

## ðŸš€ NEXT STEPS AFTER TRAINING

1. **Evaluate thoroughly**:
   ```bash
   python src/test_qwen_on_sample_v3.py
   python src/test_qwen_on_custom.py
   ```

2. **Error analysis**:
   - Review predictions tá»«ng sample
   - Identify patterns trong sai sÃ³t
   - Plan further improvements

3. **Documentation**:
   - Update final report vá»›i káº¿t quáº£ v3
   - Add augmentation methodology
   - Compare v1 vs v2 vs v3

4. **Prepare for submission**:
   - Package model + code
   - Prepare presentation slides
   - Ready for demo

---

**Sáºµn sÃ ng Ä‘á»ƒ báº¯t Ä‘áº§u? Cháº¡y lá»‡nh Ä‘áº§u tiÃªn:**
```bash
python src/augment_data.py
```

**Good luck! ðŸ€**
