# ğŸ“Š TÃ“M Táº®T: PIPELINE DATA AUGMENTATION V3

## âœ… ÄÃƒ Táº O CÃC FILES SAU:

### 1. Data Augmentation
- **`src/augment_data.py`** - Script augment training data vá»›i 5 techniques
- **`src/organize_augmented_data.py`** - Tá»• chá»©c augmented datasets

### 2. Training
- **`src/train_slm_qwen_lora_v3_augmented.py`** - Train model v3 vá»›i augmented data

### 3. Evaluation  
- **`src/test_qwen_on_sample_v3.py`** - Test trÃªn Test_sample.v1.0.csv

### 4. Documentation
- **`TRAINING_GUIDE_V3_AUGMENTED.md`** - HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ tá»« A-Z
- **`reports/improvement_strategies.md`** - Chi tiáº¿t cÃ¡c strategies tÄƒng accuracy

---

## ğŸš€ QUICK START - 5 BÆ¯á»šC ÄÆ N GIáº¢N

### BÆ°á»›c 1: Augment data (Local - 30-60 phÃºt)
```bash
source venv/bin/activate
python src/augment_data.py
python src/organize_augmented_data.py
```

### BÆ°á»›c 2: Zip & upload (5 phÃºt)
```bash
zip -r data_augmented.zip data/slm_*_augmented.jsonl
zip -r src_v3.zip src/train_slm_qwen_lora_v3_augmented.py src/model_qwen.py
# Upload to Google Colab
```

### BÆ°á»›c 3: Train trÃªn Colab (3-5 giá»)
```python
!unzip -q data_augmented.zip
!unzip -q src_v3.zip
!pip install -q transformers peft datasets accelerate
!python src/train_slm_qwen_lora_v3_augmented.py
```

### BÆ°á»›c 4: Download model (5 phÃºt)
```python
!zip -r qwen_lora_v3_augmented.zip models/qwen2.5-0.5b-med-slm-lora-v3-augmented/
from google.colab import files
files.download('qwen_lora_v3_augmented.zip')
```

### BÆ°á»›c 5: Test (Local - 10 phÃºt)
```bash
unzip qwen_lora_v3_augmented.zip
python src/test_qwen_on_sample_v3.py
```

---

## ğŸ“ˆ Káº¾T QUáº¢ Dá»° KIáº¾N

| Metric | v1 (Original) | v2 (Merged) | v3 (Augmented) | Improvement |
|--------|---------------|-------------|----------------|-------------|
| **Training samples** | 52,521 | 53,144 | **~105,000** | +100% |
| **Internal test** | 85.58% | - | **~90%** | +5% |
| **External test** | 49.76% | 69.0% | **75-85%** | +6-16% |
| **Total improvement** | Baseline | +19% | **+25-35%** | ğŸ¯ |

---

## ğŸ¯ Táº I SAO AUGMENTATION HIá»†U QUáº¢?

### 1. **Diversity** (Äa dáº¡ng cÃ¡ch diá»…n Ä‘áº¡t)
```
Original: "Insulin Ä‘Æ°á»£c sáº£n xuáº¥t bá»Ÿi tuyáº¿n tá»¥y."
Aug 1:    "Tuyáº¿n tá»¥y lÃ  cÆ¡ quan sáº£n xuáº¥t insulin."
Aug 2:    "Insulin cÃ³ nguá»“n gá»‘c tá»« tuyáº¿n tá»¥y."
Aug 3:    "Hormone insulin Ä‘Æ°á»£c tiáº¿t ra tá»« tuyáº¿n tá»¥y."
```
â†’ Model há»c Ä‘Æ°á»£c nhiá»u cÃ¡ch nÃ³i khÃ¡c nhau vá» cÃ¹ng 1 fact

### 2. **Generalization** (Tá»•ng quÃ¡t hÃ³a)
- Model khÃ´ng chá»‰ "nhá»›" 1 pattern cá»¥ thá»ƒ
- Hiá»ƒu Ä‘Æ°á»£c Ã½ nghÄ©a thá»±c sá»± cá»§a cÃ¢u
- Perform tá»‘t hÆ¡n trÃªn unseen data

### 3. **Robustness** (á»”n Ä‘á»‹nh)
- Ãt bá»‹ overfitting
- Chá»‘ng Ä‘Æ°á»£c noise trong test data
- Confidence cao hÆ¡n trong predictions

---

## ğŸ”§ AUGMENTATION TECHNIQUES

### â­ 1. Back-translation (Dá»‹ch ngÆ°á»£c)
```python
VN â†’ EN â†’ VN
"Tim cÃ³ 4 ngÄƒn" 
â†’ "Heart has 4 chambers" 
â†’ "TrÃ¡i tim cÃ³ 4 buá»“ng"
```
**Impact**: â­â­â­â­ (Ráº¥t hiá»‡u quáº£ nhÆ°ng cháº­m)

### â­ 2. Paraphrase (Diá»…n Ä‘áº¡t láº¡i)
```python
"A lÃ  B" â†’ "B lÃ  Ä‘áº·c Ä‘iá»ƒm cá»§a A"
"A cÃ³ B" â†’ "B thuá»™c vá» A"
"A gÃ¢y B" â†’ "B do A gÃ¢y ra"
```
**Impact**: â­â­â­â­â­ (Nhanh vÃ  hiá»‡u quáº£)

### â­ 3. Synonym replacement
```python
"thuá»‘c" â†’ "dÆ°á»£c pháº©m"
"Ä‘iá»u trá»‹" â†’ "chá»¯a trá»‹"
"gÃ¢y ra" â†’ "dáº«n Ä‘áº¿n"
```
**Impact**: â­â­â­ (ÄÆ¡n giáº£n, Ã­t thay Ä‘á»•i)

### â­ 4. Add medical context
```python
"Tim cÃ³ 4 ngÄƒn" 
â†’ "Trong y há»c, tim cÃ³ 4 ngÄƒn"
```
**Impact**: â­â­ (ThÃªm diversity nháº¹)

### â­ 5. Negate + flip label
```python
"Insulin háº¡ Ä‘Æ°á»ng huyáº¿t." (TRUE)
â†’ "Insulin khÃ´ng háº¡ Ä‘Æ°á»ng huyáº¿t." (FALSE)
```
**Impact**: â­â­â­â­ (Táº¡o hard negatives)

---

## âš ï¸ ÄIá»‚M QUAN TRá»ŒNG

### âœ… ÄÃšNG:
1. **Chá»‰ augment TRAINING set**
   - Validation: giá»¯ nguyÃªn original
   - Test: giá»¯ nguyÃªn original
   
2. **Test_sample.v1.0.csv hoÃ n toÃ n riÃªng biá»‡t**
   - KHÃ”NG merge vÃ o training
   - Chá»‰ dÃ¹ng Ä‘á»ƒ final evaluation
   
3. **Balance augmentation**
   - KhÃ´ng augment quÃ¡ nhiá»u (risk: noise)
   - Ratio 2-3x lÃ  optimal

### âŒ SAI:
1. âŒ Augment validation/test set
2. âŒ Merge Test_sample.v1.0.csv vÃ o train
3. âŒ Augment quÃ¡ nhiá»u láº§n (>5x)
4. âŒ KhÃ´ng kiá»ƒm tra quality cá»§a augmented data

---

## ğŸ“ SO SÃNH STRATEGIES

| Strategy | Effort | Time | Cost | Expected Gain | Recommended |
|----------|--------|------|------|---------------|-------------|
| **Augmentation** | Medium | 1 day | Free | +6-16% | â­â­â­â­â­ |
| Merge test data | Low | 1 hour | Free | +5-10% | â­â­â­â­ |
| Larger model | Low | 4 hours | Free | +10-15% | â­â­â­â­â­ |
| RAG | High | 1 week | Medium | +10-15% | â­â­â­â­ |
| Ensemble | High | 3 days | Medium | +5-10% | â­â­â­ |

**Khuyáº¿n nghá»‹**: Augmentation + Larger model = Best ROI

---

## ğŸ“Š DATASET STATS AFTER AUGMENTATION

### Before augmentation:
```
slm_train.jsonl:     52,521 samples
slm_val.jsonl:        6,565 samples
slm_test_dev.jsonl:   6,566 samples
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:               65,652 samples
```

### After augmentation:
```
slm_train_augmented.jsonl:  ~105,000 samples (2x)
slm_val_augmented.jsonl:       6,565 samples (same)
slm_test_augmented.jsonl:      6,566 samples (same)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                       ~118,000 samples
```

### Distribution of augmentation methods:
```
Original:           52,521 (50%)
Back-translate:     10,504 (10%)
Paraphrase:         15,756 (15%)
Synonym:            15,756 (15%)
Add context:        10,504 (10%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:             105,041 samples
```

---

## ğŸ” QUALITY CONTROL

### Automated checks:
```python
def quality_check(sample):
    # 1. Length check
    if len(sample['input']) < 20 or len(sample['input']) > 300:
        return False
    
    # 2. Grammar check (basic)
    if has_repeated_words(sample['input']):
        return False
    
    # 3. Label consistency
    if sample['output'] not in ['TRUE', 'FALSE']:
        return False
    
    return True
```

### Manual spot-check:
- Random sample 100 augmented samples
- Verify quality vÃ  correctness
- Remove bad samples

---

## ğŸš€ EXPECTED TIMELINE

```
Day 1 Morning:   Augmentation          (3 hours)
Day 1 Afternoon: Upload & setup Colab  (1 hour)
Day 1 Evening:   Training starts       (4-5 hours overnight)
Day 2 Morning:   Download & evaluate   (2 hours)
Day 2 Afternoon: Error analysis        (2 hours)
Day 2 Evening:   Update report         (2 hours)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:           ~14-15 hours (1.5 days)
```

---

## ğŸ¯ SUCCESS CRITERIA

### Must achieve:
- âœ… Training completes successfully
- âœ… Model size < 100MB
- âœ… Internal test accuracy > 85%
- âœ… External test accuracy > 70%

### Bonus if achieve:
- ğŸŒŸ External test accuracy > 75%
- ğŸŒŸ External test accuracy > 80%
- ğŸŒŸ External test accuracy > 85%

---

## ğŸ“ SUPPORT

Náº¿u gáº·p váº¥n Ä‘á»:

1. **Augmentation fails**: Check `src/augment_data.py` logs
2. **Training OOM**: Reduce batch size to 4
3. **Low accuracy**: Check data quality, try more epochs
4. **Colab timeout**: Save checkpoints frequently

---

**ğŸ‰ Good luck vá»›i training v3! Vá»›i augmented data, model sáº½ generalize tá»‘t hÆ¡n nhiá»u!**

**Next command:**
```bash
python src/augment_data.py
```
