"""
Script Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i model Qwen2.5-0.5B + LoRA Ä‘Ã£ train.
Cho phÃ©p nháº­p cÃ¢u há»i vÃ  xem model tráº£ lá»i TRUE/FALSE.
"""

import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ============================================================================
# CONFIGURATION
# ============================================================================

BASE_DIR = Path(__file__).parent.parent
LORA_MODEL_DIR = BASE_DIR / "models" / "qwen2.5-0.5b-med-slm-lora-v2"
BASE_MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MAX_NEW_TOKENS = 150  # TÄƒng lÃªn Ä‘á»ƒ cÃ³ Ä‘á»§ chá»— cho giáº£i thÃ­ch


# ============================================================================
# MODEL LOADING
# ============================================================================

def load_model():
    """Load model vá»›i LoRA adapter."""
    print("=" * 50)
    print("Loading Medical QA Model...")
    print("=" * 50)
    print(f"Base model: {BASE_MODEL_NAME}")
    print(f"LoRA adapter: {LORA_MODEL_DIR}")
    print(f"Device: {DEVICE}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        str(LORA_MODEL_DIR),
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Load base model
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
        device_map="auto" if DEVICE == "cuda" else None,
        trust_remote_code=True
    )
    
    # Load LoRA adapter
    model = PeftModel.from_pretrained(base_model, str(LORA_MODEL_DIR))
    model.eval()
    
    print("\nâœ“ Model loaded successfully!\n")
    return model, tokenizer


def predict(model, tokenizer, statement: str) -> dict:
    """
    Dá»± Ä‘oÃ¡n má»™t má»‡nh Ä‘á» lÃ  TRUE hay FALSE.
    
    Args:
        model: Model Ä‘Ã£ load
        tokenizer: Tokenizer
        statement: Má»‡nh Ä‘á» y khoa cáº§n kiá»ƒm tra
        
    Returns:
        dict vá»›i prediction vÃ  raw output
    """
    # Táº¡o prompt giá»‘ng lÃºc train
    prompt = f"Báº¡n lÃ  má»™t trá»£ lÃ½ y táº¿. HÃ£y tráº£ lá»i ÄÃºng hoáº·c Sai.\nNháº­n Ä‘á»‹nh: {statement}\nÄÃ¡p Ã¡n:"
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    # Decode
    generated = tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    # Extract label - kiá»ƒm tra tá»« Ä‘áº§u tiÃªn xuáº¥t hiá»‡n
    generated_upper = generated.upper()
    
    # TÃ¬m vá»‹ trÃ­ xuáº¥t hiá»‡n cá»§a cÃ¡c tá»« khÃ³a
    true_pos = float('inf')
    false_pos = float('inf')
    
    for keyword in ["TRUE", "ÄÃšNG"]:
        pos = generated_upper.find(keyword)
        if pos != -1 and pos < true_pos:
            true_pos = pos
    
    for keyword in ["FALSE", "SAI"]:
        pos = generated_upper.find(keyword)
        if pos != -1 and pos < false_pos:
            false_pos = pos
    
    # Chá»n label dá»±a trÃªn tá»« xuáº¥t hiá»‡n Ä‘áº§u tiÃªn
    if true_pos < false_pos:
        label = "TRUE"
        verdict = "âœ“ ÄÃšNG"
    elif false_pos < true_pos:
        label = "FALSE"
        verdict = "âœ— SAI"
    else:
        label = "UNKNOWN"
        verdict = "? KHÃ”NG XÃC Äá»ŠNH"
    
    return {
        "statement": statement,
        "prediction": label,
        "verdict": verdict,
        "raw_output": generated
    }


def interactive_mode(model, tokenizer):
    """Cháº¿ Ä‘á»™ tÆ°Æ¡ng tÃ¡c vá»›i ngÆ°á»i dÃ¹ng."""
    print("=" * 50)
    print("MEDICAL TRUE/FALSE QA - Interactive Mode")
    print("=" * 50)
    print("Nháº­p má»™t má»‡nh Ä‘á» y khoa Ä‘á»ƒ kiá»ƒm tra Ä‘Ãºng/sai.")
    print("GÃµ 'quit' hoáº·c 'exit' Ä‘á»ƒ thoÃ¡t.")
    print("GÃµ 'demo' Ä‘á»ƒ xem cÃ¡c vÃ­ dá»¥ máº«u.")
    print("=" * 50)
    
    while True:
        print()
        user_input = input("ğŸ“ Nháº­p má»‡nh Ä‘á»: ").strip()
        
        if not user_input:
            continue
        
        if user_input.lower() in ["quit", "exit", "q"]:
            print("\nğŸ‘‹ Táº¡m biá»‡t!")
            break
        
        if user_input.lower() == "demo":
            run_demo(model, tokenizer)
            continue
        
        # Predict
        result = predict(model, tokenizer, user_input)
        
        print(f"\n{'â”€' * 40}")
        print(f"ğŸ“‹ Má»‡nh Ä‘á»: {result['statement']}")
        print(f"ğŸ¤– Káº¿t quáº£: {result['verdict']}")
        print(f"ğŸ“„ Raw output: {result['raw_output']}")
        print(f"{'â”€' * 40}")


def run_demo(model, tokenizer):
    """Cháº¡y demo vá»›i cÃ¡c vÃ­ dá»¥ máº«u."""
    examples = [
        "Tiá»ƒu Ä‘Æ°á»ng lÃ  bá»‡nh do thiáº¿u insulin hoáº·c khÃ¡ng insulin.",
        "Uá»‘ng nhiá»u nÆ°á»›c cÃ³ thá»ƒ chá»¯a khá»i ung thÆ°.",
        "Huyáº¿t Ã¡p cao cÃ³ thá»ƒ gÃ¢y Ä‘á»™t quá»µ.",
        "KhÃ¡ng sinh cÃ³ thá»ƒ Ä‘iá»u trá»‹ Ä‘Æ°á»£c bá»‡nh cÃºm do virus.",
        "Vitamin C giÃºp tÄƒng cÆ°á»ng há»‡ miá»…n dá»‹ch.",
        "Sá»‘t lÃ  triá»‡u chá»©ng phá»• biáº¿n cá»§a nhiá»…m trÃ¹ng.",
        "Uá»‘ng bia má»—i ngÃ y tá»‘t cho tim máº¡ch.",
        "TiÃªm vaccine cÃ³ thá»ƒ gÃ¢y tá»± ká»· á»Ÿ tráº» em.",
    ]
    
    print("\n" + "=" * 50)
    print("DEMO - CÃ¡c vÃ­ dá»¥ máº«u")
    print("=" * 50)
    
    for i, statement in enumerate(examples, 1):
        result = predict(model, tokenizer, statement)
        print(f"\n{i}. {statement}")
        print(f"   â†’ {result['verdict']} (raw: {result['raw_output']})")
    
    print("\n" + "=" * 50)


def main():
    """HÃ m main."""
    # Load model
    model, tokenizer = load_model()
    
    # Cháº¡y interactive mode
    interactive_mode(model, tokenizer)


if __name__ == "__main__":
    main()
